from keras.preprocessing.image import ImageDataGenerator
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import sklearn.metrics as sk_met
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
import pickle
import tkinter as tk
from tkinter import filedialog
import cv2 as cv
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import sparse_categorical_crossentropy
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

df = pd.read_csv('twitter_training.csv')
df2 = pd.read_csv('twitter_validation.csv')
print(df.columns)
print(df2.columns)
df=df.rename(columns={'Positive':'label','im getting on borderlands and i will murder you all ,':'text'})
df2=df2.rename(columns={'Irrelevant':'label','I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tomâ€™s great auntie as â€˜Hayley canâ€™t get out of bedâ€™ and told to his grandma, who now thinks Iâ€™m a lazy, terrible person ðŸ¤£':'text'})
print(df.columns)
print(df2.columns)
trainlab=df['label'].values
vallab=df2['label'].values
traintext=df['text'].values
valtext=df2['text'].values

traintext=traintext.astype(str)
valtext=valtext.astype(str)

totaltext=np.concatenate((traintext,valtext))

tokenizer_input = Tokenizer()
tokenizer_input.fit_on_texts(totaltext)
input_sequences = tokenizer_input.texts_to_sequences(totaltext)

max_sequence_length = 50  
input_vocab_size = len(tokenizer_input.word_index) + 1

input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='post')

totallab=np.concatenate((trainlab,vallab))
totallab=totallab.reshape(totallab.shape[0],1)

enc = OneHotEncoder() 
enc_lab = enc.fit_transform(totallab).toarray()

x_train, x_test, y_train, y_test = train_test_split(input_sequences, enc_lab, test_size=0.2, random_state=42)


def translate_lstm(max_sequence_length,input_vocab_size,input_shape):
    input= tf.keras.Input(shape=input_shape[1:])
    e1= tf.keras.layers.Embedding(input_dim=input_vocab_size,output_dim=500,mask_zero=True)(input)
    lstm1= tf.keras.layers.LSTM(units=150)(e1)
    relu= tf.keras.layers.Dense(150, activation='relu')(lstm1)
    dropout= tf.keras.layers.Dropout(0.3)(relu)
    softmaxout= tf.keras.layers.Dense(4, activation='softmax')(dropout)
    model=tf.keras.Model(input,softmaxout)
    return model

def translate_rnn(max_sequence_length,input_vocab_size,input_shape):
     input= tf.keras.Input(shape=input_shape[1:])
     e1= tf.keras.layers.Embedding(input_dim=input_vocab_size,output_dim=500,mask_zero=True)(input)
     lstm1= tf.keras.layers.SimpleRNN(150)(e1)
     relu= tf.keras.layers.Dense(150, activation='relu')(lstm1)
     dropout= tf.keras.layers.Dropout(0.3)(relu)
     softmaxout= tf.keras.layers.Dense(4, activation='softmax')(dropout)
     model=tf.keras.Model(input,softmaxout)
     return model
 
def translate_gru(max_sequence_length,input_vocab_size,input_shape):
     input= tf.keras.Input(shape=input_shape[1:])
     e1= tf.keras.layers.Embedding(input_dim=input_vocab_size,output_dim=500,mask_zero=True)(input)
     lstm1= tf.keras.layers.GRU(150)(e1)
     relu= tf.keras.layers.Dense(150, activation='relu')(lstm1)
     dropout= tf.keras.layers.Dropout(0.3)(relu)
     softmaxout= tf.keras.layers.Dense(4, activation='softmax')(dropout)
     model=tf.keras.Model(input,softmaxout)
     return model
 
def plot_training_history(history):
    plt.figure(figsize=(12, 6))

    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('Model accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend(['Train', 'Validation'], loc='upper left')

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend(['Train', 'Validation'], loc='upper left')

    plt.tight_layout()
    plt.show()

model=translate_lstm(max_sequence_length,input_vocab_size,x_train.shape) 
model.compile(optimizer='Adam',metrics=['accuracy'],loss='categorical_crossentropy')
model.summary()

model=translate_gru(max_sequence_length,input_vocab_size,x_train.shape) 
model.compile(optimizer='Adam',metrics=['accuracy'],loss='categorical_crossentropy')
model.summary()

model=translate_rnn(max_sequence_length,input_vocab_size,x_train.shape) 
model.compile(optimizer='Adam',metrics=['accuracy'],loss='categorical_crossentropy')
model.summary()



